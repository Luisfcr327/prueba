{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbebea1e",
   "metadata": {},
   "source": [
    "# Remesas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "638b925d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerías\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import pyodbc\n",
    "import numpy as np\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import NamedStyle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe4d2fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RECORDATORIO: ¿Ya cambiaste el timedelta del Lunes o Miércoles/Viernes?. Presiona ENTER para continuar\n",
      "RECORDATORIO: ¿Ya actualizaste el número de remesa correspondiente?. Presiona ENTER para continuar\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input(\"RECORDATORIO: ¿Ya cambiaste el timedelta del Lunes o Miércoles/Viernes?. Presiona ENTER para continuar\")\n",
    "input(\"RECORDATORIO: ¿Ya actualizaste el número de remesa correspondiente?. Presiona ENTER para continuar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf051849",
   "metadata": {},
   "source": [
    "### IMPORTANTE: Actualiza los datos de abajo cada día"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16f0ba7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINIR FECHAS\n",
    "# remesa de los lunes editar timedelta(days=2)\n",
    "# remesa de miercoles y viernes dejar timedelta(days=1)\n",
    "#dia_hoy = (datetime.now() - timedelta(days=6)).strftime('%d%m%Y') # Utilizar en caso de remesa omitida\n",
    "\n",
    "dia_hoy = datetime.now().strftime('%d%m%Y')\n",
    "dia_ayer = (datetime.now() - timedelta(days=1)).strftime('%d%m%Y')\n",
    "\n",
    "# DEFINIR NUMERO DE REMESAS\n",
    "num_bradescard = '289'\n",
    "num_copayment = '561'\n",
    "num_prosa = '792'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "660e02cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta de la carpeta de descargas (ahí se genera el archivo)\n",
    "carpeta_descargas = os.path.join(os.path.expanduser('~'), 'Downloads')\n",
    "\n",
    "# Nombre del archivo Excel que deseas crear\n",
    "pre_remesa = f'Pre remesa {dia_hoy}.xlsx'\n",
    "\n",
    "# Crear la ruta completa del archivo Pre remesa\n",
    "path_archivo = os.path.join(carpeta_descargas, pre_remesa)\n",
    "\n",
    "# Crear un DataFrame (df)\n",
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d784a39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre del archivo Excel que deseas crear\n",
    "bradescard = f'Lote de aclaraciones {num_bradescard}.xlsx'\n",
    "# Crear la ruta completa del archivo Lote de aclaraciones\n",
    "path_archivo_b = os.path.join(carpeta_descargas, bradescard)\n",
    "\n",
    "# Nombre del archivo Excel que deseas crear\n",
    "copayment = f'Remesa {num_copayment}.xlsx'\n",
    "# Crear la ruta completa del archivo Remesa copayment\n",
    "path_archivo_c = os.path.join(carpeta_descargas, copayment)\n",
    "\n",
    "# Nombre del archivo Excel que deseas crear\n",
    "prosa = f'Remesa {num_prosa}.xlsx'\n",
    "# Crear la ruta completa del archivo Remesa prosa\n",
    "path_archivo_p = os.path.join(carpeta_descargas, prosa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96d8068f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conexión exitosa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yordi.ruelas\\AppData\\Local\\Temp\\ipykernel_9500\\2019052220.py:171: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(consulta_sql, connection)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conexión cerrada\n"
     ]
    }
   ],
   "source": [
    "# Conexión a la base de datos\n",
    "server = \"MP-VW-DB-021\"  # Tu servidor\n",
    "database = \"BONIFINMED\"  # Tu base de datos\n",
    "\n",
    "conn_str = (\n",
    "    f\"DRIVER={{ODBC Driver 17 for SQL Server}};\"\n",
    "    f\"SERVER={server};\"\n",
    "    f\"DATABASE={database};\"\n",
    "    f\"Trusted_Connection=yes;\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    # Mensaje que indica si la conexión es exitosa\n",
    "    connection = pyodbc.connect(conn_str)\n",
    "    print(\"Conexión exitosa\")\n",
    "    \n",
    "    # Calcular las fechas de inicio y fin de la consulta\n",
    "    hoy = datetime.now()\n",
    "    hace_20_dias = hoy - timedelta(days=20)\n",
    "\n",
    "    fecha_hoy_str = hoy.strftime('%Y-%m-%d')\n",
    "    fecha_hace_20_dias_str = hace_20_dias.strftime('%Y-%m-%d')\n",
    "\n",
    "    # Consulta de SQL Procesos_Bonificación_Inmediata_VF 2.0\n",
    "    consulta_sql = f'''\n",
    "    WITH FilasEnumeradas AS (\n",
    "      SELECT\n",
    "        CASE\n",
    "          WHEN tblTransaccionHistorica.nombreArchivo <> ('SitioWeb') THEN 'Falcon'\n",
    "          ELSE tblTransaccionHistorica.nombreArchivo\n",
    "        END AS Ingreso,\n",
    "        tblTransaccionHistorica.duplicidad510,\n",
    "        tblTransaccionHistorica.nombreArchivo,\n",
    "        tblTransaccionHistorica.tarjeta, \n",
    "        tblTransaccionHistorica.ID_Auto, \n",
    "        tblTransaccionHistorica.importeTran, \n",
    "        tblTransaccionHistorica.fechaTran, \n",
    "        tblProsaTemp.Hr, \n",
    "        tblProsaTemp.estatusCompra,\n",
    "        tblTransaccionReglaBonificacion.importe AS Monto_Bonificado, \n",
    "        CASE\n",
    "          WHEN TIPO_TRANSACCION IN ('01') THEN 'Transacción compra'\n",
    "          WHEN TIPO_TRANSACCION IN ('21') THEN 'Transacción Devolución'\n",
    "          WHEN TIPO_TRANSACCION IS NULL AND tblFlujoLevantamiento2.tipoFraude IN ('ATM','Transacción ATM con categoria diferente a ATM') THEN 'Disposición en efectivo'\n",
    "          WHEN TIPO_TRANSACCION IS NULL AND tblTransaccionReglaBonificacion.ticket IS NULL THEN 'No posteado'\n",
    "          WHEN TIPO_TRANSACCION IS NULL AND reglaICE_ECI IN ('=') THEN 'Transacción compra'\n",
    "          ELSE TIPO_TRANSACCION\n",
    "        END AS Tipo_Transacción,\n",
    "        tblTransaccionReferencia.referencia AS Referencia,\n",
    "        tblProsaTemp.TERM_CNTRY_CDE AS Pais,\n",
    "        tblTransaccionHistorica.Nombre_Cmr AS Nombre_Comercio, \n",
    "        SUBSTRING(tblProsaTemp.PT_SRV_ENTRY_MDE, 1, 2) AS PEM, \n",
    "        (tblFlujoLevantamiento2.reglaICE_ECI + tblFlujoLevantamiento2.iCE_ECI) AS ICE,\n",
    "        CASE\n",
    "          WHEN tblFlujoLevantamiento2.tipoFraude IS NULL THEN 'No posteado'\n",
    "          ELSE tblTransaccionReglaBonificacion.ticket\n",
    "        END AS Ticket_GLPI,\n",
    "        [dbo].[tmpTranFolio].FOLIO_BI,\n",
    "        [dbo].[tmpTranFolio].TIPO_ACLARACIÓN,\n",
    "        CASE\n",
    "          WHEN tblFlujoLevantamiento2.tipoFraude IS NULL THEN 'No posteada'\n",
    "          ELSE tblFlujoLevantamiento2.tipoFraude\n",
    "        END AS Categoria_Regla,\n",
    "        CASE\n",
    "          WHEN descripcionBonificar IS NULL THEN 'No se bonifica'\n",
    "          ELSE descripcionBonificar\n",
    "        END AS Regla_Asignada,\n",
    "        CASE\n",
    "          WHEN tblTransaccionReglaBonificacion.tipobonificacion IS NULL THEN 'No se Bonifica'\n",
    "          ELSE tblTransaccionReglaBonificacion.tipobonificacion\n",
    "        END AS Tipo_Bonificación,\n",
    "        CASE\n",
    "          WHEN tblTransaccionReglaBonificacion.estatusAsignado IS NULL THEN 'No posteada'\n",
    "          ELSE tblTransaccionReglaBonificacion.estatusAsignado\n",
    "        END AS Estatus_Asignado,\n",
    "        tblProsaTemp.fechaAct AS Fecha_Ult_Estatus_Act,\n",
    "        CASE\n",
    "          WHEN tblTransaccionReglaBonificacion.aplicaTC40 IS NULL THEN 'NO'\n",
    "          ELSE tblTransaccionReglaBonificacion.aplicaTC40\n",
    "        END AS Aplica_TC40,\n",
    "        CASE\n",
    "          WHEN tblTransaccionReglaBonificacion.aplicaRemesa IS NULL THEN 'NO'\n",
    "          ELSE tblTransaccionReglaBonificacion.aplicaRemesa\n",
    "        END AS Aplica_Remesa,\n",
    "        tblTransaccionHistorica.fechaAct AS Fecha_marcado, \n",
    "        [BONIFICACION].[dbo].[tblFolio].fechaAlta,\n",
    "        BONIFICACION.dbo.tblFolio.comentario,\n",
    "        BONIFICACION.dbo.tblFolio.statementDay,\n",
    "        tblTransaccionReglaBonificacion.fechaAct AS Fecha_Proceso,\n",
    "        tblRemesa2.nombreArchivo AS Nombre_Remesa,\n",
    "        tblTransaccionReglaBonificacion.estatusTicket,\n",
    "        ROW_NUMBER() OVER (PARTITION BY tblTransaccionHistorica.tarjeta, tblTransaccionHistorica.id_auto ORDER BY tblTransaccionHistorica.fechaTran DESC) AS NumFila\n",
    "      FROM\n",
    "        tblTransaccionHistorica WITH (NOLOCK) \n",
    "      LEFT JOIN \n",
    "        tblTransaccionReglaBonificacion WITH (NOLOCK) ON tblTransaccionHistorica.tarjeta = tblTransaccionReglaBonificacion.tarjeta AND tblTransaccionHistorica.ID_Auto = tblTransaccionReglaBonificacion.apprv\n",
    "      LEFT JOIN \n",
    "        tbl510POSHistorial WITH (NOLOCK) ON tblTransaccionHistorica.tarjeta = tbl510POSHistorial.NUM_CUENTA AND tblTransaccionHistorica.ID_Auto = tbl510POSHistorial.NUM_AUT\n",
    "      LEFT JOIN \n",
    "        tblFlujoLevantamiento2 WITH (NOLOCK) ON tblTransaccionReglaBonificacion.idFlujoLevantamiento = tblFlujoLevantamiento2.idFlujoLevantamiento\n",
    "      LEFT JOIN \n",
    "        tbl510ATMHistorial WITH (NOLOCK) ON tblTransaccionHistorica.tarjeta = tbl510ATMHistorial.NumeroDTarjeta AND tblTransaccionHistorica.ID_Auto = tbl510ATMHistorial.Autorización\n",
    "      LEFT JOIN\n",
    "        tblProsaTemp WITH (NOLOCK) ON tblTransaccionHistorica.tarjeta = tblProsaTemp.NUM AND tblTransaccionHistorica.ID_Auto = tblProsaTemp.APPRV_CDE\n",
    "      LEFT JOIN  \n",
    "        [dbo].[tmpTranFolio] WITH (NOLOCK) ON tblTransaccionHistorica.tarjeta = [dbo].[tmpTranFolio].NUM AND tblTransaccionHistorica.ID_Auto = [dbo].[tmpTranFolio].autorizacion\n",
    "      LEFT JOIN \n",
    "        tblTransaccionReferencia WITH (NOLOCK) ON tblTransaccionHistorica.tarjeta = tblTransaccionReferencia.tarjeta AND tblTransaccionHistorica.ID_Auto = tblTransaccionReferencia.apprv\n",
    "      LEFT JOIN \n",
    "        BONIFICACION.dbo.tblFolio WITH (NOLOCK) ON [dbo].[tmpTranFolio].FOLIO_BI = BONIFICACION.dbo.tblFolio.folio\n",
    "      LEFT JOIN \n",
    "        tblRemesa2 WITH (NOLOCK) ON tblTransaccionReglaBonificacion.tarjeta = tblRemesa2.TARJETA AND tblTransaccionReglaBonificacion.apprv = tblRemesa2.AUT \n",
    "    )\n",
    "    SELECT\n",
    "      CASE\n",
    "        WHEN Nombre_Comercio = 'COMPRA BRADES WEB APP' THEN 'Weconnect NO ENVIAR A REMESA'\n",
    "        ELSE 'Flujo Normal'\n",
    "      END AS Marcador_Weconnect,\n",
    "      Ingreso,\n",
    "      Fecha_Proceso,\n",
    "      duplicidad510,\n",
    "      fechaAlta AS 'Fecha Interacción',\n",
    "      tarjeta,\n",
    "      ID_Auto,\n",
    "      importeTran,\n",
    "      Monto_Bonificado,\n",
    "      fechaTran,\n",
    "      Hr,\n",
    "      Referencia,\n",
    "      Pais,\n",
    "      Nombre_Comercio,\n",
    "      PEM,\n",
    "      ICE,\n",
    "      CASE \n",
    "        WHEN Ticket_GLPI IS NULL THEN 'Pendiente Alta de ticket'\n",
    "        ELSE Ticket_GLPI \n",
    "      END AS Ticket_GLPI,\n",
    "      estatusTicket,\n",
    "      Folio_BI,\n",
    "      TIPO_ACLARACIÓN,\n",
    "      Categoria_Regla,\n",
    "      Regla_Asignada,\n",
    "      CASE\n",
    "        WHEN Estatus_Asignado = 'Sin regla asignada' THEN 'No se Bonifica'\n",
    "        ELSE Tipo_Bonificación\n",
    "      END AS Tipo_Bonificación,\n",
    "      Estatus_Asignado,\n",
    "      Aplica_TC40,\n",
    "      Aplica_Remesa,\n",
    "      Nombre_Remesa,\n",
    "      Comentario,\n",
    "      statementDay,\n",
    "      CASE\n",
    "        WHEN Estatus_Asignado IN ('Bonificadas','Sin regla asignada','En análisis de fraude') AND Aplica_Remesa IN ('SI') THEN 'Cuenta en análisis, en espera de contracargo/dictamen'\n",
    "        WHEN Estatus_Asignado IN ('En análisis de fraude') AND ICE IN ('=5') THEN 'Validar VCAS/en espera de dictamen' \n",
    "        WHEN Estatus_Asignado IN ('Quebranto') THEN 'Validar Accesorios y Cerrar ticket a favor del cliente'\n",
    "        WHEN Estatus_Asignado IN ('No posteada') THEN 'Transacción No posteada'\n",
    "        WHEN Regla_Asignada LIKE ('%total%') THEN 'Devolución por parte del comercio, cerrar ticket'\n",
    "        WHEN Categoria_Regla IN ('Me la robaron/la extravíe') THEN 'Bloqueo R, Improcedente por políticas Internas'\n",
    "        WHEN Categoria_Regla IN ('No hice el trámite de esa tarjeta (Robo de identidad)','Cancelación no atendida','Doble emboso','Pre aprobados','Primeras compras en agregadores','Reposición/Adicional no reconocida','Tarjeta no entregada') THEN 'Realizar Predictamen, Canalizar con Fraudes'\n",
    "      END AS Dictamen\n",
    "    FROM\n",
    "      FilasEnumeradas WITH (NOLOCK)\n",
    "    WHERE \n",
    "      numFila = 1  \n",
    "      AND fechaAlta >= '{fecha_hace_20_dias_str}' AND fechaAlta < '{fecha_hoy_str} 09:00:00.000'\n",
    "    ORDER BY [Fecha Interacción] DESC\n",
    "    '''\n",
    "    \n",
    "    # Ejecutar la consulta y cargar los datos en df\n",
    "    df = pd.read_sql(consulta_sql, connection)\n",
    "    \n",
    "    # Convertir todas las columnas a texto (string)\n",
    "    df = df.astype(str)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error al ejecutar la consulta o la conexión: \", e)\n",
    "\n",
    "finally:\n",
    "    # Cerrar la conexión\n",
    "    connection.close()\n",
    "    print(\"Conexión cerrada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66b8278e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear la columna 'Llave' concatenando 'tarjeta' y 'ID_Auto'\n",
    "df['Llave'] = df['tarjeta'] + df['ID_Auto']\n",
    "df['Llave'] = df['Llave'].astype(str)\n",
    "\n",
    "# Crear la columna 'Escenario'\n",
    "df['Escenario'] = ''\n",
    "\n",
    "# Reorganizar las columnas para que 'Llave' y 'Escenario' estén al principio\n",
    "df = df[['Llave', 'Escenario'] + [col for col in df.columns if col not in ['Llave', 'Escenario']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df73be5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Especifica la ruta completa al archivo de Excel Consolidado BI 2025\n",
    "ruta_archivo_excel = r'\\\\celerrabr\\Compartido\\Entre_Sesiones\\D7627S086\\Aclaracion y Prev de Fraudes\\MIS\\Remesas\\Remesas Historico\\2025\\Consolidado_Pre_Remesas_2025.xlsx'  # Cambia esta ruta según sea necesario\n",
    "hoja_nombre = 'Consolidado BI 2025'\n",
    "\n",
    "# Cargar la hoja en un DataFrame llamado df_consolidado\n",
    "df_consolidado = pd.read_excel(ruta_archivo_excel, sheet_name=hoja_nombre)\n",
    "df_consolidado['Llave tarj & Aut'] = df_consolidado['Llave tarj & Aut'].astype(str)\n",
    "\n",
    "# Crear llave en df_consolidado\n",
    "df['Llave'] = df['Llave'].str.strip()\n",
    "df_consolidado['Llave tarj & Aut'] = df_consolidado['Llave tarj & Aut'].str.strip()\n",
    "\n",
    "df['Llave'] = df['Llave'].str.replace(r'\\s+', '', regex=True)\n",
    "df_consolidado['Llave tarj & Aut'] = df_consolidado['Llave tarj & Aut'].str.replace(r'\\s+', '', regex=True)\n",
    "\n",
    "df = df.drop_duplicates(subset=['Llave'])\n",
    "df_consolidado = df_consolidado.drop_duplicates(subset=['Llave tarj & Aut'])\n",
    "\n",
    "# Usar una máscara para encontrar todas las filas que contienen la palabra 'prueba' (ignora mayúsculas y minúsculas)\n",
    "# mask_prueba = df['Comentario'].str.contains('prueba', case=False, na=False)\n",
    "\n",
    "# Actualizar la columna Escenario en las filas donde se encuentra 'prueba' en la columna Comentario\n",
    "#df.loc[mask_prueba, 'Escenario'] = 'Ticket prueba productiva'\n",
    "\n",
    "# Hacer el cruce\n",
    "df = df.merge(df_consolidado[['Llave tarj & Aut']], \n",
    "              left_on='Llave', right_on='Llave tarj & Aut', \n",
    "              how='left', indicator=True)\n",
    "\n",
    "# Marcamos como \"Ya trabajados\" solo las filas que cruzaron con el consolidado\n",
    "df['Escenario'] = df.apply(lambda row: 'Ya trabajados' if row['_merge'] == 'both' else row['Escenario'], axis=1)\n",
    "\n",
    "# Eliminamos la columna '_merge' que se genera por el merge\n",
    "df = df.drop(columns=['_merge'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0ce5aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escenario Weconnect\n",
    "df['Escenario'] = df.apply(\n",
    "    lambda row: 'Weconnect' if (\n",
    "        pd.isna(row['Escenario']) or row['Escenario'] == ''  # Escenario está vacío\n",
    "        and row['Marcador_Weconnect'] in ['Weconnect NO ENVIAR A REMESA']\n",
    "    ) else row['Escenario'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Escenario FNZ*ESC XHIDEL COMERCI\n",
    "df['Escenario'] = df.apply(\n",
    "    lambda row: 'FNZ*ESC XHIDEL COMERCI' if (\n",
    "        pd.isna(row['Escenario']) or row['Escenario'] == ''  # Escenario está vacío\n",
    "        and row['Nombre_Comercio'] in ['FNZ*ESC XHIDEL COMERCI']\n",
    "    ) else row['Escenario'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Escenario No posteado\n",
    "df['Escenario'] = df.apply(\n",
    "    lambda row: 'No posteado' if (\n",
    "        pd.isna(row['Escenario']) or row['Escenario'] == ''  # Escenario está vacío\n",
    "        and row['Ticket_GLPI'] in ['No posteado']\n",
    "    ) else row['Escenario'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Escenario Pendiente Alta de ticket\n",
    "df['Escenario'] = df.apply(\n",
    "    lambda row: 'Pendiente Alta de ticket' if (\n",
    "        pd.isna(row['Escenario']) or row['Escenario'] == ''  # Escenario está vacío\n",
    "        and row['Ticket_GLPI'] in ['Pendiente Alta de ticket']\n",
    "    ) else row['Escenario'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Escenario Se trabaja en archivo siguiente\n",
    "df['Escenario'] = df.apply(\n",
    "    lambda row: 'Se trabaja en archivo siguiente' if (\n",
    "        pd.isna(row['Escenario']) or row['Escenario'] == ''  # Escenario está vacío\n",
    "        and row['Ticket_GLPI'] in ['']\n",
    "    ) else row['Escenario'],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb9fb6cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ruta de archivo de las remesas\n",
    "ruta_remesa = r'\\\\celerrabr\\Reportes\\BonificacionInmediata\\Archivos\\Remesa'\n",
    "\n",
    "# Nombre de las remesas a descargar\n",
    "remesa_hoy = f'''Remesa_{dia_hoy}.xls'''\n",
    "remesa_ayer = f'''Remesa_{dia_ayer}.xls'''\n",
    "\n",
    "# Descargar remesas\n",
    "ruta_remesa_hoy = os.path.join(ruta_remesa, remesa_hoy)\n",
    "df_remesa_hoy = pd.read_excel(\n",
    "    ruta_remesa_hoy,\n",
    "    dtype={\n",
    "        'AUT': str,\n",
    "        'MCC': str,\n",
    "        'IMPORTE': str,\n",
    "        'PEM': str,\n",
    "        'AFILIACION': str,\n",
    "        'ICE': str,\n",
    "        'TOKEN_Q2': str\n",
    "    }, \n",
    "    engine='openpyxl'\n",
    ")\n",
    "\n",
    "ruta_remesa_ayer = os.path.join(ruta_remesa, remesa_ayer)\n",
    "df_remesa_ayer = pd.read_excel(\n",
    "    ruta_remesa_ayer,\n",
    "    dtype={\n",
    "        'AUT': str,\n",
    "        'MCC': str,\n",
    "        'IMPORTE': str,\n",
    "        'PEM': str,\n",
    "        'AFILIACION': str,\n",
    "        'ICE': str,\n",
    "        'TOKEN_Q2': str\n",
    "    }, \n",
    "    engine='openpyxl'\n",
    ")\n",
    "\n",
    "# unir las dos remesas en un mismo DataFrame\n",
    "df_remesas = pd.concat([df_remesa_hoy, df_remesa_ayer], ignore_index=True)\n",
    "df_remesas = df_remesas.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "397c6536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear Llave para df_remesas\n",
    "df_remesas['Llave'] = df_remesas['TARJETA'] + df_remesas['AUT']\n",
    "df_remesas['Llave'] = df_remesas['Llave'].astype(str)\n",
    "\n",
    "# Añadir columnas a df_remesas\n",
    "df_remesas['Comentario y # remesa'] = ''\n",
    "df_remesas['Proveedor'] = ''\n",
    "\n",
    "df_remesas = df_remesas[['Llave', 'Comentario y # remesa', 'Proveedor'] + [col for col in df_remesas.columns if col not in ['Llave', 'Comentario y # remesa', 'Proveedor']]]\n",
    "\n",
    "llaves_remesa = df_remesas['Llave'].unique()  # Lista de llaves únicas en df_remesas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03c1eb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escenario Enviados a remesa\n",
    "df['Escenario'] = df.apply(\n",
    "    lambda row: 'Enviados a remesa' if (pd.isna(row['Escenario']) or row['Escenario'] == '') and row['Llave'] in llaves_remesa else row['Escenario'], \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Escenario Monto menor\n",
    "df['Escenario'] = df.apply(\n",
    "    lambda row: 'Monto menor' if (\n",
    "        pd.isna(row['Escenario']) or row['Escenario'] == '' # Escenario está vacío\n",
    "        and row['TIPO_ACLARACIÓN'] in ['Compra no reconocida', 'Primera compra CNP', 'Devolución no aplicada', 'No pasó y pagó con otro medio'] # Condiciones en TIPO_ACLARACIÓN\n",
    "        and 'monto menor' in str(row['Regla_Asignada']).lower() # Condición en Regla_Asignada (insensible a mayúsculas)\n",
    "    ) else row['Escenario'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Escenario ICE 5\n",
    "df['Escenario'] = df.apply(\n",
    "    lambda row: 'ICE 5' if (\n",
    "        pd.isna(row['Escenario']) or row['Escenario'] == ''  # Escenario está vacío\n",
    "        and row['TIPO_ACLARACIÓN'] in ['Compra no reconocida', 'Primera compra CNP']  # Condiciones en TIPO_ACLARACIÓN\n",
    "        and row['ICE'] == '=5'  # Condición en ICE\n",
    "    ) else row['Escenario'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Escenario Devolución Total\n",
    "df['Escenario'] = df.apply(\n",
    "    lambda row: 'Devolución Total' if (\n",
    "        pd.isna(row['Escenario']) or row['Escenario'] == ''  # Escenario está vacío\n",
    "        and row['TIPO_ACLARACIÓN'] in ['Compra no reconocida', 'Primera compra CNP']  # Condiciones en TIPO_ACLARACIÓN\n",
    "        and row['Regla_Asignada'] == 'Si se identifica que la devolución es total '  # Condición en ICE\n",
    "    ) else row['Escenario'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Escenario Bloqueo R\n",
    "df['Escenario'] = df.apply(\n",
    "    lambda row: 'Bloqueo R' if (\n",
    "        pd.isna(row['Escenario']) or row['Escenario'] == ''  # Escenario está vacío\n",
    "        and row['TIPO_ACLARACIÓN'] in ['Me la robaron/la extravíe']  # Condiciones en TIPO_ACLARACIÓN\n",
    "    ) else row['Escenario'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Escenario TNR\n",
    "df['Escenario'] = df.apply(\n",
    "    lambda row: 'TNR' if (\n",
    "        pd.isna(row['Escenario']) or row['Escenario'] == ''  # Escenario está vacío\n",
    "        and row['TIPO_ACLARACIÓN'] in ['Cancelación no atendida', 'Reposición/Adicional no reconocida', 'Tarjeta no entregada', 'No hice el trámite de esa tarjeta (Robo de identidad)', 'Doble emboso', 'Pre aprobados', 'Cargos adicionales no autorizados por mí']  # Condiciones en TIPO_ACLARACIÓN\n",
    "    ) else row['Escenario'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Escenario Aclaración Incorrecta\n",
    "df['Escenario'] = df.apply(\n",
    "    lambda row: 'Aclaración Incorrecta' if (\n",
    "        pd.isna(row['Escenario']) or row['Escenario'] == ''  # Escenario está vacío\n",
    "        and row['TIPO_ACLARACIÓN'] in ['Cargo duplicado']  # Condiciones en TIPO_ACLARACIÓN\n",
    "    ) else row['Escenario'],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da2e13d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar PRUEBAS de remesas\n",
    "#filtro_prueba = df_remesas['COMENTARIO_TKT'].str.contains(r'prueba|peurba', case=False, na=False)\n",
    "\n",
    "#df_remesas = df_remesas[~filtro_prueba]\n",
    "\n",
    "# Eliminar Weconnect de remesas\n",
    "llaves_weconnect = df[df['Escenario'] == 'Weconnect']['Llave']\n",
    "\n",
    "df_remesas = df_remesas[~df_remesas['Llave'].isin(llaves_weconnect)]\n",
    "\n",
    "# Eliminar FNZ*ESC XHIDEL COMERCI de remesas\n",
    "llaves_xhideli = df[df['Escenario'] == 'FNZ*ESC XHIDEL COMERCI']['Llave']\n",
    "\n",
    "df_remesas = df_remesas[~df_remesas['Llave'].isin(llaves_xhideli)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e9577e1-c926-4401-8c94-33c254b450b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 Casos con 00 remplazados\n"
     ]
    }
   ],
   "source": [
    "#Corregir los Token_Q2 que sean igual a 00\n",
    "\n",
    "#Lee el Catalogo de Q2 que esta en descargas formato excel\n",
    "ruta_descargas = os.path.join(os.path.expanduser(\"~\"), \"Downloads\")\n",
    "nombre_archivo = \"Validacion Q2-0.xlsx\"\n",
    "ruta_completa = os.path.join(ruta_descargas, nombre_archivo)\n",
    "\n",
    "# Leer Excel y convertirlo a diccionario\n",
    "df_catalogo = pd.read_excel(ruta_completa, dtype={0: str, 1: str})\n",
    "dic = dict(zip(df_catalogo.iloc[:, 0], df_catalogo.iloc[:, 1]))\n",
    "\n",
    "df_token_cero = df_remesas[df_remesas[\"TOKEN_Q2\"] == \"00\"]\n",
    "cantidad = len(df_token_cero)\n",
    "\n",
    "\n",
    "#Cambiar el Q2 00 por el que corresponda\n",
    "df_remesas.loc[df_remesas['TOKEN_Q2'] == '00', 'TOKEN_Q2'] = \\\n",
    "df_remesas.loc[df_remesas['TOKEN_Q2'] == '00', 'AFILIACION'].map(dic).fillna('00')\n",
    "\n",
    "df_token_cero = df_remesas[df_remesas[\"TOKEN_Q2\"] == \"00\"]\n",
    "cantidad_nueva = len(df_token_cero)\n",
    "cantidad = cantidad - cantidad_nueva\n",
    "cantidad = str(cantidad)\n",
    "\n",
    "print(cantidad + \" Casos con 00 remplazados\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "429f16fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** Corrección de error TIPO_BLOQUEO > Fraude Digitado, COD_PAIS > MX, TOKEN_Q2 > 03, PEM > != 01 10, TIPO_BLOQUEO = Clonación\n",
    "df_remesas['TIPO_BLOQUEO'] = df_remesas.apply(\n",
    "    lambda row: 'Clonación' if (\n",
    "        row['TIPO_BLOQUEO'] in ['Fraude Digitado']\n",
    "        and row['COD_PAIS'] in ['MX']\n",
    "        and row['TOKEN_Q2'] in ['03', '04']\n",
    "        and row['PEM'] in ['02', '05', '07', '80', '90']\n",
    "    ) else row['TIPO_BLOQUEO'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# ** Correccion de error Compra no reconocida y PEM None = Compra no reconocida y PEM 01\n",
    "df['PEM'] = df.apply(\n",
    "    lambda row: '01' if (\n",
    "        row['TIPO_ACLARACIÓN'] in ['Compra no reconocida']\n",
    "        and row['PEM'] in ['None']\n",
    "    ) else row['PEM'],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fca1abe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proveedor BRADESCARD\n",
    "df_remesas['Proveedor'] = df_remesas.apply(\n",
    "    lambda row: 'BRADESCARD' if (\n",
    "        pd.isna(row['Proveedor']) or row['Proveedor'] == ''\n",
    "        and row['TIPO_BLOQUEO'] in ['Fraude Digitado']\n",
    "        and row['COD_PAIS'] in ['MX']\n",
    "        and row['TOKEN_Q2'] in ['02', '08', '09']\n",
    "    ) else row['Proveedor'],\n",
    "    axis=1\n",
    ")\n",
    "# Comentario y # remesa Lote de aclaraciones XXX\n",
    "df_remesas['Comentario y # remesa'] = df_remesas.apply(\n",
    "    lambda row: f'Lote de aclaraciones {num_bradescard}' if (\n",
    "        pd.isna(row['Comentario y # remesa']) or row['Comentario y # remesa'] == ''\n",
    "        and row['TIPO_BLOQUEO'] in ['Fraude Digitado']\n",
    "        and row['COD_PAIS'] in ['MX']\n",
    "        and row['TOKEN_Q2'] in ['02', '08', '09']\n",
    "    ) else row['Comentario y # remesa'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Proveedor y Comentario y # remesa PEM 07\n",
    "df_remesas['Proveedor'] = df_remesas.apply(\n",
    "    lambda row: 'PEM 07' if (\n",
    "        pd.isna(row['Proveedor']) or row['Proveedor'] == ''\n",
    "        and row['TIPO_BLOQUEO'] in ['Clonación']\n",
    "        and row['COD_PAIS'] in ['MX']\n",
    "        and row['PEM'] in ['07']\n",
    "    ) else row['Proveedor'],\n",
    "    axis=1\n",
    ")\n",
    "df_remesas['Comentario y # remesa'] = df_remesas.apply(\n",
    "    lambda row: 'PEM 07' if (\n",
    "        pd.isna(row['Comentario y # remesa']) or row['Comentario y # remesa'] == ''\n",
    "        and row['TIPO_BLOQUEO'] in ['Clonación']\n",
    "        and row['COD_PAIS'] in ['MX']\n",
    "        and row['PEM'] in ['07']\n",
    "    ) else row['Comentario y # remesa'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Proveedor y Comentario y # remesa Clonación Internacional\n",
    "df_remesas['Proveedor'] = df_remesas.apply(\n",
    "    lambda row: 'Clonación internacional' if (\n",
    "        pd.isna(row['Proveedor']) or row['Proveedor'] == ''\n",
    "        and row['TIPO_BLOQUEO'] in ['Clonación']\n",
    "        and row['COD_PAIS'] not in ['MX']\n",
    "    ) else row['Proveedor'],\n",
    "    axis=1\n",
    ")\n",
    "df_remesas['Comentario y # remesa'] = df_remesas.apply(\n",
    "    lambda row: 'Clonación internacional' if (\n",
    "        pd.isna(row['Comentario y # remesa']) or row['Comentario y # remesa'] == ''\n",
    "        and row['TIPO_BLOQUEO'] in ['Clonación']\n",
    "        and row['COD_PAIS'] not in ['MX']\n",
    "    ) else row['Comentario y # remesa'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Proveedor PROSA\n",
    "df_remesas['Proveedor'] = df_remesas.apply(\n",
    "    lambda row: 'PROSA' if (\n",
    "        pd.isna(row['Proveedor']) or row['Proveedor'] == ''\n",
    "        and row['TIPO_BLOQUEO'] in ['Clonación', 'ATM']\n",
    "        and row['COD_PAIS'] in ['MX', 'None']\n",
    "    ) else row['Proveedor'],\n",
    "    axis=1\n",
    ")\n",
    "# Comentario y # remesa XXX (prosa)\n",
    "df_remesas['Comentario y # remesa'] = df_remesas.apply(\n",
    "    lambda row: num_prosa if (\n",
    "        pd.isna(row['Comentario y # remesa']) or row['Comentario y # remesa'] == ''\n",
    "        and row['TIPO_BLOQUEO'] in ['Clonación', 'ATM']\n",
    "        and row['COD_PAIS'] in ['MX', 'None']\n",
    "    ) else row['Comentario y # remesa'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Proveedor COPAYMENT\n",
    "df_remesas['Proveedor'] = df_remesas.apply(\n",
    "    lambda row: 'COPAYMENT' if (\n",
    "        pd.isna(row['Proveedor']) or row['Proveedor'] == ''\n",
    "    ) else row['Proveedor'],\n",
    "    axis=1\n",
    ")\n",
    "# Comentario y # remesa XXX (copayment)\n",
    "df_remesas['Comentario y # remesa'] = df_remesas.apply(\n",
    "    lambda row: num_copayment if (\n",
    "        pd.isna(row['Comentario y # remesa']) or row['Comentario y # remesa'] == ''\n",
    "    ) else row['Comentario y # remesa'],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b1bd6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar los registros de df_remesas donde el proveedor es PEM 07\n",
    "df_remesas_pem07 = df_remesas[df_remesas['Proveedor'] == 'PEM 07']\n",
    "# Actualizar la columna 'Escenario' en df cuando las 'Llave' coinciden y el proveedor es PEM 07\n",
    "df['Escenario'] = df.apply(\n",
    "    lambda row: 'PEM 07' if row['Llave'] in df_remesas_pem07['Llave'].values and row['Escenario'] not in ['Ya trabajados'] else row['Escenario'], axis=1)\n",
    "# Eliminar los registros de df_remesas donde el proveedor es PEM 07\n",
    "df_remesas = df_remesas[df_remesas['Proveedor'] != 'PEM 07']\n",
    "\n",
    "# Filtrar los registros de df_remesas donde el proveedor es Clonación internacional\n",
    "df_remesas_CI = df_remesas[df_remesas['Proveedor'] == 'Clonación internacional']\n",
    "# Actualizar la columna 'Escenario' en df cuando las 'Llave' coinciden y el proveedor es PEM 07\n",
    "df['Escenario'] = df.apply(\n",
    "    lambda row: 'Clonación internacional' if row['Llave'] in df_remesas_CI['Llave'].values and row['Escenario'] not in ['Ya trabajados'] else row['Escenario'], axis=1)\n",
    "# Eliminar los registros de df_remesas donde el proveedor es PEM 07\n",
    "df_remesas = df_remesas[df_remesas['Proveedor'] != 'Clonación internacional']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65ea02ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escenario Se trabaja en archivo siguiente\n",
    "df['Escenario'] = df['Escenario'].replace('', 'Se trabaja en archivo siguiente')\n",
    "\n",
    "# Nuevas columnas para df \n",
    "df['Fecha trabajada'] = ''\n",
    "df['Comentario y # remesa'] = ''\n",
    "df['Proveedor'] = ''\n",
    "df['Tipo Bloqueo'] = ''\n",
    "\n",
    "# Reorganizar las columnas para colocar las nuevas después de 'Llave' y 'Escenario'\n",
    "columnas_ordenadas = (\n",
    "    df.columns.tolist()[:2]  # Primeras dos columnas (Llave y Escenario)\n",
    "    + ['Fecha trabajada', 'Comentario y # remesa', 'Proveedor', 'Tipo Bloqueo']  # Nuevas columnas\n",
    "    + df.columns.tolist()[2:-4]  # Resto de las columnas, excluyendo las 4 nuevas\n",
    ")\n",
    "\n",
    "# Reorganizar df con el nuevo orden\n",
    "df = df[columnas_ordenadas]\n",
    "\n",
    "# Eliminar columna de Marcador_Weconncet de df\n",
    "df = df.drop('Marcador_Weconnect', axis=1)\n",
    "# Eliminar columna de Llave tarj & Aut de df\n",
    "df = df.drop('Llave tarj & Aut', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b50b493",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fecha trabajada dia_hoy1\n",
    "dia_hoy1 = datetime.now().strftime('%d/%m/%Y')\n",
    "\n",
    "df['Fecha trabajada'] = df.apply(\n",
    "    lambda row: dia_hoy1 if row['Escenario'] in ['Bloqueo R', 'Devolución Total', 'Enviados a remesa', 'ICE 5', 'Monto menor', 'PEM 07', 'Ticket prueba productiva', 'TNR', 'Weconnect', 'Clonación internacional', 'Aclaración Incorrecta', 'FNZ*ESC XHIDEL COMERCI'] else row['Fecha trabajada'], \n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc1cca24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cruce de Comentario y # remesa de df_remesas\n",
    "# Crear el diccionario de 'Comentario y # remesa' excluyendo las filas donde Escenario es \"Ya trabajados\"\n",
    "comentario_dict = df_remesas[['Llave', 'Comentario y # remesa']].drop_duplicates().set_index('Llave')['Comentario y # remesa'].to_dict()\n",
    "\n",
    "# Actualizar 'Comentario y # remesa' solo si Escenario no es \"Ya trabajados\"\n",
    "df['Comentario y # remesa'] = df.apply(\n",
    "    lambda row: comentario_dict.get(row['Llave'], row['Comentario y # remesa']) \n",
    "    if row['Escenario'] != 'Ya trabajados' else row['Comentario y # remesa'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Cruce de Proveedor de df_remesas a df\n",
    "# Crear el diccionario de 'Proveedor' excluyendo las filas donde Escenario es \"Ya trabajados\"\n",
    "proveedor_dict = df_remesas[['Llave', 'Proveedor']].drop_duplicates().set_index('Llave')['Proveedor'].to_dict()\n",
    "\n",
    "# Actualizar 'Proveedor' solo si Escenario no es \"Ya trabajados\"\n",
    "df['Proveedor'] = df.apply(\n",
    "    lambda row: proveedor_dict.get(row['Llave'], row['Proveedor']) \n",
    "    if row['Escenario'] != 'Ya trabajados' else row['Proveedor'],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5c372a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cruce de Comentario y # remesa de df_remesas a df\n",
    "#comentario_dict = df_remesas[['Llave', 'Comentario y # remesa']].drop_duplicates().set_index('Llave')['Comentario y # remesa'].to_dict()\n",
    "\n",
    "#df['Comentario y # remesa'] = df['Llave'].map(comentario_dict).combine_first(df['Comentario y # remesa'])\n",
    "\n",
    "# Cruce de Proveedor de df_remesas a df\n",
    "#proveedor_dict = df_remesas[['Llave', 'Proveedor']].drop_duplicates().set_index('Llave')['Proveedor'].to_dict()\n",
    "\n",
    "#df['Proveedor'] = df['Llave'].map(proveedor_dict).combine_first(df['Proveedor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f5bf6c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proveedor NO SE ENVIAN\n",
    "df['Proveedor'] = df.apply(\n",
    "    lambda row: 'NO SE ENVIAN' if row['Escenario'] in ['Bloqueo R', 'Devolución Total', 'ICE 5', 'Monto menor', 'PEM 07', 'Ticket prueba productiva', 'TNR', 'Weconnect', 'Clonación internacional', 'Primeras compras en agregadores', 'Primeras compras en agregadores', 'Aclaración Incorrecta', 'FNZ*ESC XHIDEL COMERCI'] else row['Proveedor'], \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Comentario y # remesa es igual a Escenario si Proveedor es NO SE ENVIAN\n",
    "df['Comentario y # remesa'] = df.apply(\n",
    "    lambda row: row['Escenario'] if row['Proveedor'] == 'NO SE ENVIAN' else row['Comentario y # remesa'],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d85e172b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tipo Bloqueo es igual a TIPO_ACLARACIÓN si es distinto de Compra no reconocida o Primera compra CNP\n",
    "df['Tipo Bloqueo'] = df.apply(\n",
    "    lambda row: row['TIPO_ACLARACIÓN'] if row['Tipo Bloqueo'] == '' and row['Fecha trabajada'] != '' and pd.notna(row['Fecha trabajada']) and row['TIPO_ACLARACIÓN'] not in ['Compra no reconocida', 'Primera compra CNP'] else row['Tipo Bloqueo'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Tipo Bloqueo Fraude Digitado\n",
    "df['Tipo Bloqueo'] = df.apply(\n",
    "    lambda row: 'Fraude Digitado' if row['Fecha trabajada'] != '' and pd.notna(row['Fecha trabajada']) and row['TIPO_ACLARACIÓN'] in ['Compra no reconocida', 'Primera compra CNP'] and row['PEM'] in ['01', '10'] else row['Tipo Bloqueo'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Tipo Bloqueo Clonación\n",
    "df['Tipo Bloqueo'] = df.apply(\n",
    "    lambda row: 'Clonación' if row['Fecha trabajada'] != '' and pd.notna(row['Fecha trabajada']) and row['TIPO_ACLARACIÓN'] in ['Compra no reconocida', 'Primera compra CNP'] and row['PEM'] in ['02', '05', '07', '80', '90'] and row['Pais'] in ['MX'] else row['Tipo Bloqueo'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Tipo Bloqueo Clonación internacional\n",
    "df['Tipo Bloqueo'] = df.apply(\n",
    "    lambda row: 'Clonación internacional' if row['Fecha trabajada'] != '' and pd.notna(row['Fecha trabajada']) and row['TIPO_ACLARACIÓN'] in ['Compra no reconocida', 'Primera compra CNP'] and row['PEM'] in ['02', '05', '07', '80', '90'] and row['Pais'] not in ['MX'] else row['Tipo Bloqueo'],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4fa1db1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar filas duplicadas basándose en la columna 'Llave', conservando solo la primera ocurrencia\n",
    "df_remesas = df_remesas.drop_duplicates(subset='Llave', keep='first').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6fafdd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar las filas donde Proveedor es igual a 'BRADESCARD'\n",
    "df_bradescard = df_remesas[\n",
    "    df_remesas['Proveedor'] == 'BRADESCARD'\n",
    "][[\n",
    "    'TARJETA', 'COMENTARIO_TKT', 'REFERENCIA', 'TIPO_BLOQUEO', \n",
    "    'TICKET', 'FECHA_TICKET', 'MCC', 'COD_PAIS', 'IMPORTE', \n",
    "    'F_TRX', 'AUT', 'PEM', 'NOMBRE_DEL_COMERCIO', 'MERCHANT_CITY', \n",
    "    'AFILIACION', 'ICE', 'TOKEN_Q2', 'CLIENTE'\n",
    "]]\n",
    "\n",
    "# Filtrar las filas donde Proveedor es igual a 'BRADESCARD'\n",
    "df_copayment = df_remesas[\n",
    "    df_remesas['Proveedor'] == 'COPAYMENT'\n",
    "][[\n",
    "    'TARJETA', 'COMENTARIO_TKT', 'REFERENCIA', 'TIPO_BLOQUEO', \n",
    "    'TICKET', 'FECHA_TICKET', 'MCC', 'COD_PAIS', 'IMPORTE', \n",
    "    'F_TRX', 'AUT', 'PEM', 'NOMBRE_DEL_COMERCIO', 'MERCHANT_CITY', \n",
    "    'AFILIACION', 'ICE', 'TOKEN_Q2', 'CLIENTE'\n",
    "]]\n",
    "\n",
    "# Filtrar las filas donde Proveedor es igual a 'BRADESCARD'\n",
    "df_prosa = df_remesas[\n",
    "    df_remesas['Proveedor'] == 'PROSA'\n",
    "][[\n",
    "    'TARJETA', 'COMENTARIO_TKT', 'REFERENCIA', 'TIPO_BLOQUEO', \n",
    "    'TICKET', 'FECHA_TICKET', 'MCC', 'COD_PAIS', 'IMPORTE', \n",
    "    'F_TRX', 'AUT', 'PEM', 'NOMBRE_DEL_COMERCIO', 'MERCHANT_CITY', \n",
    "    'AFILIACION', 'ICE', 'TOKEN_Q2', 'CLIENTE'\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "60c0427e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El archivo principal 'Pre remesa 15102025.xlsx' ha sido creado. Realiza los cambios necesarios y guarda el archivo.\n",
      "Recuerda realizar las VALIDACIONES que se detallan MÁS ABAJO\n",
      "Presiona Enter cuando hayas terminado de realizar los cambios en el archivo principal...\n",
      "\n",
      "Los archivos adicionales y las hojas en el archivo principal han sido generados correctamente.\n"
     ]
    }
   ],
   "source": [
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import NamedStyle\n",
    "\n",
    "# Paso 1: Generar el archivo principal\n",
    "df.to_excel(path_archivo, index=False)\n",
    "\n",
    "# Exportar inicialmente df_remesas a una hoja separada\n",
    "with pd.ExcelWriter(path_archivo, engine='openpyxl', mode='a') as writer:\n",
    "    df_remesas.to_excel(writer, sheet_name='Remesa', index=False)\n",
    "\n",
    "print(f\"El archivo principal '{pre_remesa}' ha sido creado. Realiza los cambios necesarios y guarda el archivo.\")\n",
    "print('Recuerda realizar las VALIDACIONES que se detallan MÁS ABAJO')\n",
    "input(\"Presiona Enter cuando hayas terminado de realizar los cambios en el archivo principal...\")\n",
    "\n",
    "# Paso 2: Leer el archivo principal modificado\n",
    "df = pd.read_excel(path_archivo, dtype=str)\n",
    "df_remesas = pd.read_excel(path_archivo, sheet_name='Remesa', dtype=str)\n",
    "\n",
    "# Si necesitas actualizar otros DataFrames basados en los cambios realizados:\n",
    "df_bradescard = df_remesas[df_remesas['Proveedor'] == 'BRADESCARD']\n",
    "df_copayment = df_remesas[df_remesas['Proveedor'] == 'COPAYMENT']\n",
    "df_prosa = df_remesas[df_remesas['Proveedor'] == 'PROSA']\n",
    "\n",
    "# Filtrar las filas donde Proveedor es igual a los valores correspondientes\n",
    "columnas_seleccionadas = [\n",
    "    'TARJETA', 'COMENTARIO_TKT', 'REFERENCIA', 'TIPO_BLOQUEO', \n",
    "    'TICKET', 'FECHA_TICKET', 'MCC', 'COD_PAIS', 'IMPORTE', \n",
    "    'F_TRX', 'AUT', 'PEM', 'NOMBRE_DEL_COMERCIO', 'MERCHANT_CITY', \n",
    "    'AFILIACION', 'ICE', 'TOKEN_Q2', 'CLIENTE'\n",
    "]\n",
    "\n",
    "df_bradescard = df_bradescard[columnas_seleccionadas]\n",
    "df_copayment = df_copayment[columnas_seleccionadas]\n",
    "df_prosa = df_prosa[columnas_seleccionadas]\n",
    "\n",
    "# Paso 3: Crear nuevas hojas en el archivo principal\n",
    "with pd.ExcelWriter(path_archivo, engine='openpyxl', mode='a') as writer:\n",
    "    df_bradescard.to_excel(writer, sheet_name=f'Lote de aclaraciones {num_bradescard}', index=False)\n",
    "    df_copayment.to_excel(writer, sheet_name=num_copayment, index=False)\n",
    "    df_prosa.to_excel(writer, sheet_name=num_prosa, index=False)\n",
    "\n",
    "# Aplicar formato de texto a las hojas generadas\n",
    "wb = load_workbook(path_archivo)\n",
    "\n",
    "# Definir estilo de texto\n",
    "text_style = NamedStyle(name=\"text\", number_format=\"@\")\n",
    "\n",
    "for sheet_name in [f'Lote de aclaraciones {num_bradescard}', num_copayment, num_prosa]:\n",
    "    ws = wb[sheet_name]\n",
    "    for row in ws.iter_rows():\n",
    "        for cell in row:\n",
    "            cell.style = text_style  # Aplicar formato de texto\n",
    "\n",
    "wb.save(path_archivo)\n",
    "\n",
    "# Crear archivos separados para cada proveedor y aplicar formato texto\n",
    "for archivo, df_separado in zip([path_archivo_b, path_archivo_c, path_archivo_p], \n",
    "                                 [df_bradescard, df_copayment, df_prosa]):\n",
    "    with pd.ExcelWriter(archivo, engine='openpyxl') as writer:\n",
    "        df_separado.to_excel(writer, index=False)\n",
    "    \n",
    "    # Aplicar formato texto a cada archivo\n",
    "    wb = load_workbook(archivo)\n",
    "    ws = wb.active\n",
    "    for row in ws.iter_rows():\n",
    "        for cell in row:\n",
    "            cell.style = text_style  # Aplicar formato de texto\n",
    "    wb.save(archivo)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Los archivos adicionales y las hojas en el archivo principal han sido generados correctamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2876a000",
   "metadata": {},
   "source": [
    "\n",
    "**VALIDAR:**\n",
    "\n",
    "1.ATM (disposición de efectivo) que empiece por vuXXXX se envia PROSA tipo de bloqueo disposición de efectivo\n",
    "\n",
    "2.Duplicados\n",
    "\n",
    "3.BRADESCARD Fraudes Digitados solo mayores a 50 si son mx\n",
    "\n",
    "4.PEM NULL en Tipo Bloqueo\n",
    "\n",
    "5.Texto en columnas para fecha trabajada"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
